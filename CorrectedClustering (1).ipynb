{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzxVA76t8ac9",
        "outputId": "ab8c82d2-86ae-4277-86bf-8d3688ebc02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: striprtf in /usr/local/lib/python3.10/dist-packages (0.0.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install striprtf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHKxWQbQ8eef",
        "outputId": "b29a00fe-ea10-4b91-a10e-61e872e24f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3nN62Q2R8gJg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.cluster import KMeans\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "from sklearn.metrics import silhouette_score\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DLGQd7-k8l-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0670021a-ca15-4b78-9d28-5b8140c6d594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riKsxSYYW4Fu"
      },
      "source": [
        "PREPROCESSING PART:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pMuuBgMT8snP"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "directory = r\"/content/drive/MyDrive/WordDataset/Q1\"\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".rtf\"):\n",
        "        with open(os.path.join(directory, filename), \"rb\") as file:\n",
        "            rtf_text = file.read().decode(\"utf-8\")\n",
        "            # Convert .rtf to plain text\n",
        "            text = rtf_to_text(rtf_text)\n",
        "            text = text.lower()\n",
        "            # Tokenize and preprocess\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "            # print(tokens)\n",
        "            # break\n",
        "\n",
        "            words = [word.lower() for word in tokens if word.isalpha()]\n",
        "            words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "            cleaned_text = \" \".join(words)\n",
        "            # print(\"['\", cleaned_text, sep=\"\")\n",
        "            documents.append(cleaned_text)\n",
        "            # print(documents)\n",
        "            # break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix6KpM9-W-Eg"
      },
      "source": [
        "CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7u9Rm0syXBmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044691d9-9528-45e3-b747-ba2b8a4bb755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.2588658010866576\n",
            "Document 1: Cluster 0\n",
            "Document 2: Cluster 0\n",
            "Document 3: Cluster 0\n",
            "Document 4: Cluster 0\n",
            "Document 5: Cluster 0\n",
            "Document 6: Cluster 0\n",
            "Document 7: Cluster 0\n",
            "Document 8: Cluster 0\n",
            "Document 9: Cluster 0\n",
            "Document 10: Cluster 0\n",
            "Document 11: Cluster 0\n",
            "Document 12: Cluster 0\n",
            "Document 13: Cluster 0\n",
            "Document 14: Cluster 0\n",
            "Document 15: Cluster 0\n",
            "Document 16: Cluster 0\n",
            "Document 17: Cluster 0\n",
            "Document 18: Cluster 0\n",
            "Document 19: Cluster 0\n",
            "Document 20: Cluster 0\n",
            "Document 21: Cluster 0\n",
            "Document 22: Cluster 0\n",
            "Document 23: Cluster 0\n",
            "Document 24: Cluster 0\n",
            "Document 25: Cluster 0\n",
            "Document 26: Cluster 0\n",
            "Document 27: Cluster 0\n",
            "Document 28: Cluster 0\n",
            "Document 29: Cluster 0\n",
            "Document 30: Cluster 0\n",
            "Document 31: Cluster 0\n",
            "Document 32: Cluster 0\n",
            "Document 33: Cluster 0\n",
            "Document 34: Cluster 0\n",
            "Document 35: Cluster 0\n",
            "Document 36: Cluster 0\n",
            "Document 37: Cluster 0\n",
            "Document 38: Cluster 0\n",
            "Document 39: Cluster 0\n",
            "Document 40: Cluster 0\n",
            "Document 41: Cluster 0\n",
            "Document 42: Cluster 0\n",
            "Document 43: Cluster 0\n",
            "Document 44: Cluster 0\n",
            "Document 45: Cluster 0\n",
            "Document 46: Cluster 0\n",
            "Document 47: Cluster 0\n",
            "Document 48: Cluster 0\n",
            "Document 49: Cluster 0\n",
            "Document 50: Cluster 0\n",
            "Document 51: Cluster 0\n",
            "Document 52: Cluster 0\n",
            "Document 53: Cluster 0\n",
            "Document 54: Cluster 3\n",
            "Document 55: Cluster 0\n",
            "Document 56: Cluster 0\n",
            "Document 57: Cluster 0\n",
            "Document 58: Cluster 0\n",
            "Document 59: Cluster 0\n",
            "Document 60: Cluster 0\n",
            "Document 61: Cluster 0\n",
            "Document 62: Cluster 0\n",
            "Document 63: Cluster 0\n",
            "Document 64: Cluster 0\n",
            "Document 65: Cluster 0\n",
            "Document 66: Cluster 0\n",
            "Document 67: Cluster 0\n",
            "Document 68: Cluster 0\n",
            "Document 69: Cluster 0\n",
            "Document 70: Cluster 0\n",
            "Document 71: Cluster 0\n",
            "Document 72: Cluster 0\n",
            "Document 73: Cluster 0\n",
            "Document 74: Cluster 2\n",
            "Document 75: Cluster 0\n",
            "Document 76: Cluster 0\n",
            "Document 77: Cluster 0\n",
            "Document 78: Cluster 4\n",
            "Document 79: Cluster 0\n",
            "Document 80: Cluster 0\n",
            "Document 81: Cluster 0\n",
            "Document 82: Cluster 0\n",
            "Document 83: Cluster 0\n",
            "Document 84: Cluster 0\n",
            "Document 85: Cluster 0\n",
            "Document 86: Cluster 0\n",
            "Document 87: Cluster 1\n",
            "Document 88: Cluster 0\n",
            "Document 89: Cluster 0\n",
            "Document 90: Cluster 0\n",
            "Document 91: Cluster 0\n",
            "Document 92: Cluster 0\n",
            "Document 93: Cluster 0\n",
            "Document 94: Cluster 0\n",
            "Document 95: Cluster 0\n",
            "Document 96: Cluster 0\n",
            "Document 97: Cluster 0\n",
            "Document 98: Cluster 0\n",
            "Document 99: Cluster 0\n",
            "Document 100: Cluster 0\n",
            "Document 101: Cluster 0\n",
            "Document 102: Cluster 0\n",
            "Document 103: Cluster 0\n",
            "Document 104: Cluster 0\n",
            "Document 105: Cluster 0\n"
          ]
        }
      ],
      "source": [
        "print(len(documents))\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, max_df=1)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(documents)\n",
        "num_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(tfidf_vectors)\n",
        "silhouette_avg = silhouette_score(tfidf_vectors, cluster_labels)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "for i, label in enumerate(cluster_labels):\n",
        "    print(f\"Document {i+1}: Cluster {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANZ1U3gSIuMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}